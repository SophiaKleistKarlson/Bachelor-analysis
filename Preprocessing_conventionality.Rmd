---
title: "Preprocessing Conventionality"
author: "Sophia Kleist Karlson"
date: "22 nov 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)


pacman::p_load(tidyverse, jsonlite, rjson, stringr, dplyr)

setwd("~/Social Transmission Study/Analysis of drawings/")

RStudio.Version() #why this?
```




importing conventionality data round 1
```{r}

#specifying data path
data_path_m <- 'data/mturk/round_1' 

#creating a list of files
list_files_m <- list.files(path = data_path_m,
                         recursive = T,
                         pattern = "session.json$",
                         full.names = T)

data_path_m
list_files_m

length(list_files_m)

# empty data fame with the correct column names
all_data_m <- data.frame(matrix(ncol = 1, nrow = 0)) #ncol doesn't actually make a difference



# extract all mturk files and 
for (i in 1:length(list_files_m)){
  file_path_m <- list_files_m[i]
  d_m <- jsonlite::fromJSON(file_path_m, flatten=T) %>% 
    select(drawing_id, button_pressed, completion_code) %>% 
    rename(Drawing_ID = drawing_id,
           Conventionality = button_pressed) %>% 
    filter(is.na(Drawing_ID) + is.na(completion_code) < 2 ,)
  if (nrow(all_data_m) == 0){
    all_data_m <- d_m
  } else {
    all_data_m <- rbind(all_data_m, d_m)
  }
}

class(all_data_m$Conventionality)
all_data_m$Conventionality <- as.numeric(all_data_m$Conventionality)
mean(all_data_m$Conventionality)
sd(all_data_m$Conventionality)
range(all_data_m$Conventionality)

completion_codes <- all_data_m$completion_code 
completion_codes <- completion_codes[complete.cases(completion_codes)]




#merge conventionality scores with all_data
all_data_w_convention_1 <- merge(all_data, all_data_m)
all_data_w_conv_comp <- merge(all_data_2, all_data_m)

all_data_w_convention_1$completion_code <- NULL 
all_data_w_convention_1$completion_code <- NULL 

#write csv files
write.csv(all_data_w_convention_1, "data/csv_files/all_data_w_convention.csv") # all data with conventionality scores
write.csv(completion_codes, "data/csv_files/completion_codes.csv") # completion codes
write.csv(all_data_m, "data/csv_files/conventionality_data.csv") # conventionality scores
write.csv(all_data_w_conv_comp, "data/csv_files/all_data_w_conv_comp.csv") # all data with conventionality and complexity scores


```



importing conventionality data from round 2
```{r}

#specifying data path
data_path_m_2 <- 'data/mturk/round_2' 

#creating a list of files
list_files_m_2 <- list.files(path = data_path_m_2,
                         recursive = T,
                         pattern = "session.json$",
                         full.names = T)

data_path_m_2
list_files_m_2
length(list_files_m_2)

# empty data fame with the correct column names
all_data_m_2 <- data.frame(matrix(ncol = 1, nrow = 0)) #ncol doesn't actually make a difference



# extract all mturk files and 
for (i in 1:length(list_files_m_2)){
  file_path_m_2 <- list_files_m_2[i]
  d_m_2 <- jsonlite::fromJSON(file_path_m_2, flatten=T) %>% 
    select(drawing_id, button_pressed, completion_code) %>% 
    rename(Drawing_ID = drawing_id,
           Conventionality = button_pressed) %>% 
    filter(is.na(Drawing_ID) + is.na(completion_code) < 2 ,)
  if (nrow(all_data_m_2) == 0){
    all_data_m_2 <- d_m_2
  } else {
    all_data_m_2 <- rbind(all_data_m_2, d_m_2)
  }
}

class(all_data_m_2$Conventionality)
all_data_m_2$Conventionality <- as.numeric(all_data_m_2$Conventionality)
mean(all_data_m_2$Conventionality)
sd(all_data_m_2$Conventionality)
range(all_data_m_2$Conventionality)

completion_codes_2 <- all_data_m_2$completion_code 
completion_codes_2 <- completion_codes_2[complete.cases(completion_codes_2)]


#all_data_m_2$completion_code <- NULL
#all_data_m$completion_code <- NULL

# rbind conventionality 1 and conventionality 2 scores
all_conventionality <- rbind(all_data_m_2, all_data_m)
all_conventionality$completion_code <- NULL

```




making rows for source images for each chain - we use all_data_2
```{r}

# make new dataframe from all_data_2
all_data_w_source <- all_data_2

# add 240 empty rows to be filled with source images in the beginning of each chain
all_data_w_source[nrow(all_data_w_source)+240,] <- NA

# check class of generation column
class(all_data_w_source$Generation)

# make every generation 1 higher
all_data_w_source <- all_data_w_source %>% mutate(Generation = Generation +1)



# read csv with complexity of source images
source_comp <- read.csv("data/source_images/complexity/complexity_comparison_source.csv")
source_comp$X <- NULL

# add generation column which is 0
source_comp$Generation <- rep(0,12)

# add source image column, which is just the drawing id without "stim_"
source_comp <- source_comp %>% mutate(Source_image = str_replace_all(Drawing_ID, 'stim_', ''))

# make drawing id into character
class(source_comp$Drawing_ID)
source_comp$Drawing_ID <- as.character(source_comp$Drawing_ID)


# add columns from source_comp to all_data_w_source
all_data_w_source[1681:1692,1] <- source_comp$Drawing_ID
all_data_w_source[1681:1692,7] <- source_comp$Generation
all_data_w_source[1681:1692,9] <- source_comp$Source_image
all_data_w_source[1681:1692,15] <- source_comp$Complexity_original
all_data_w_source[1681:1692,16] <- source_comp$Complexity_convolution

# add chain id
all_data_w_source[1681:1692,6] <- rep(0,12)

# not very tidy, but it works
all_data_w_source[1693:1704,] <- all_data_w_source[1681:1692,]

# column 6 is chain
all_data_w_source[1693:1704,6] <- rep(12,12) # chain 12

# do the same with the rest of the source image rows:
all_data_w_source[1705:1716,] <- all_data_w_source[1681:1692,]
all_data_w_source[1705:1716,6] <- rep(13,12) # chain 13

all_data_w_source[1717:1728,] <- all_data_w_source[1681:1692,]
all_data_w_source[1717:1728,6] <- rep(14,12) # chain 14

all_data_w_source[1729:1740,] <- all_data_w_source[1681:1692,]
all_data_w_source[1729:1740,6] <- rep(15,12) # chain 15
                  
all_data_w_source[1741:1752,] <- all_data_w_source[1681:1692,]
all_data_w_source[1741:1752,6] <- rep(16,12)

all_data_w_source[1753:1764,] <- all_data_w_source[1681:1692,]
all_data_w_source[1753:1764,6] <- rep(17,12)

all_data_w_source[1765:1776,] <- all_data_w_source[1681:1692,]
all_data_w_source[1765:1776,6] <- rep(18,12)

all_data_w_source[1777:1788,] <- all_data_w_source[1681:1692,]
all_data_w_source[1777:1788,6] <- rep(19,12)

all_data_w_source[1789:1800,] <- all_data_w_source[1681:1692,]
all_data_w_source[1789:1800,6] <- rep(2,12)

all_data_w_source[1801:1812,] <- all_data_w_source[1681:1692,]
all_data_w_source[1801:1812,6] <- rep(20,12)

all_data_w_source[1813:1824,] <- all_data_w_source[1681:1692,]
all_data_w_source[1813:1824,6] <- rep(21,12)

all_data_w_source[1825:1836,] <- all_data_w_source[1681:1692,]
all_data_w_source[1825:1836,6] <- rep(22,12)

all_data_w_source[1837:1848,] <- all_data_w_source[1681:1692,]
all_data_w_source[1837:1848,6] <- rep(23,12)

all_data_w_source[1849:1860,] <- all_data_w_source[1681:1692,]
all_data_w_source[1849:1860,6] <- rep(3,12)

all_data_w_source[1861:1872,] <- all_data_w_source[1681:1692,]
all_data_w_source[1861:1872,6] <- rep(4,12)

all_data_w_source[1873:1884,] <- all_data_w_source[1681:1692,]
all_data_w_source[1873:1884,6] <- rep(5,12)

all_data_w_source[1885:1896,] <- all_data_w_source[1681:1692,]
all_data_w_source[1885:1896,6] <- rep(7,12)

all_data_w_source[1897:1908,] <- all_data_w_source[1681:1692,]
all_data_w_source[1897:1908,6] <- rep(8,12)

all_data_w_source[1909:1920,] <- all_data_w_source[1681:1692,]
all_data_w_source[1909:1920,6] <- rep(9,12)


for (i in 1:1920){
  # if generation is 0 (which is only true for source image rows)
  for (c in 0:23){
    for (s in 1:12){
      if (all_data_w_source[i, 6] == c & all_data_w_source[i, 7] == 0 & all_data_w_source[i, 9] == s){
        for (j in 1:1920){
          if (all_data_w_source[j, 6] == c & all_data_w_source[j, 7] == 1 & all_data_w_source[j, 9] == s){
            all_data_w_source[i, 8] <- all_data_w_source[j, 8]
          }
        }
      }
    }
  }
}

all_data_w_all_conv_source <- merge(all_conventionality, all_data_w_source)
write.csv(all_data_w_all_conv_source, "data/csv_files/all_data_w_all_conv_source.csv")
```



note that these below are without complexity and conventionality scores of the source images!
```{r}



# note that these are without the source images!!!

#merge conventionality scores with all_data
all_data_w_all_convention <- merge(all_conventionality, all_data) # note that this is without the source images!
all_data_w_conv_comp_2 <- merge(all_conventionality, all_data_2)

all_data_w_all_convention$completion_code <- NULL 
all_data_w_conv_comp_2$completion_code <- NULL 

#write csv files

write.csv(all_conventionality, "data/csv_files/all_conventionality.csv")
write.csv(all_data_w_all_convention, "data/csv_files/all_data_w_all_convention.csv") # all data with conventionality scores
write.csv(completion_codes_2, "data/csv_files/completion_codes_2.csv") # completion codes
write.csv(all_data_m_2, "data/csv_files/conventionality_data_2.csv") # conventionality scores
write.csv(all_data_w_conv_comp_2, "data/csv_files/all_data_w_conv_comp_2.csv") # all data with conventionality and complexity scores


```



looking at mean and sd of the data in different ways
```{r}
all_data_m1 <- data.frame(matrix(ncol = 1, nrow = 0)) #ncol doesn't actually make a difference

for (i in 1:length(list_files_m)){
  file_path_m <- list_files_m[i]
  d_m <- jsonlite::fromJSON(file_path_m, flatten=T) %>% 
    #select(drawing_id, button_pressed, completion_code) %>% 
    rename(Drawing_ID = drawing_id,
           Conventionality = button_pressed) %>% 
    filter(is.na(Drawing_ID) + is.na(completion_code) < 2 ,)
  if (nrow(all_data_m1) == 0){
    all_data_m1 <- d_m
  } else {
    all_data_m1 <- rbind(all_data_m1, d_m)
  }
}

all_data_m1$Conventionality <- as.numeric(all_data_m1$Conventionality)
class(all_data_m1$Conventionality)


# grouping by participant and looking at the last 35 images
conv_last35 <- all_data_m1 %>% 
  group_by(subject) %>%   
  slice(tail(row_number(), 35)) %>% 
  select(Conventionality, subject) %>% 
  as.data.frame()

mean(conv_last35$Conventionality) #4.606845 - so basically the same as the overall mean
sd(conv_last35$Conventionality) #2.965397 - also basically the same - both mean and sd go up 0.1 but it doesn't seem problematic


# grouping by participant
conv_by_subject <- all_data_m1 %>% 
  group_by(subject) %>%
  select(Conventionality, subject) %>% 
  summarise(mean(Conventionality), sd(Conventionality)) %>% 
  as.data.frame()

conv_by_subject

range(conv_by_subject[,3]) #range of sd's of scores for each image goes from 0.6206257 to 3.9582781 - people are generally rating images higher than others are
mean(conv_by_subject[,3]) # mean sd og each participant is 2.087293, so they each are relatively stable in their trend


# grouping by image
conv_by_image <- all_data_m1 %>% 
  group_by(Drawing_ID) %>%   
  select(Conventionality, Drawing_ID) %>% 
  summarise(mean(Conventionality), sd(Conventionality)) %>% 
  as.data.frame()


conv_by_image <- conv_by_image[-c(1693), ] # deleitng the empty row (don't know where it came from, but it's all empty)

range(conv_by_image[,3]) #range of sd's of scores for each image goes from 0.834523 to 4.274091 - so for some images, people are not agreeing, for others, they are 
mean(conv_by_image[,3]) # mean sd of each image is 2.723376, so around the same as the overall sd

```

